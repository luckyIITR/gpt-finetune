{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b92de21e-c258-427b-b7b7-73f167cdca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access environment variables\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d57d41c-4033-46f0-9dd7-d4f4242786e4",
   "metadata": {},
   "source": [
    "## Directly using pretrained model using pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a8c4efb-862c-4176-895c-43a9329c4320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26db179d-7475-421d-831c-411c4b0efe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de892887-fbd8-4fd4-9cee-55147ed26da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Today I will teach you a few basic principles for getting rid of your phone for a few days.\\n\\n\\n1. Phone the most important thing you need to do\\n2. Do everything you need to\\n3. Have the right to do'}]\n"
     ]
    }
   ],
   "source": [
    "print(pipe(\"Today I will teach you\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3105ea-1b61-4bc1-9ecf-7a027ccfaa9d",
   "metadata": {},
   "source": [
    "There are two types of language modeling, causal and masked. GPT-2 is an example of a causal language model. \\\n",
    "Causal language modeling predicts the next token in a sequence of tokens, and the model can only attend to tokens on the left.\n",
    "\n",
    "Will finetine distilbert/distilgpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a87b8-a350-478f-9339-6d58ff0eaef8",
   "metadata": {},
   "source": [
    "Will be using rexarski/eli5_category dataset for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "364b4efc-e095-485a-b985-f6d8d38f9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3d7a73f-a4f2-4ef4-93dd-c8a9b14b6954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: eli5_category/default\n",
      "Reusing dataset eli5_category (/Users/lucky/.cache/huggingface/datasets/rexarski___eli5_category/default/1.0.0/80106cc49322f1f5075e1387be4a5b74b95e0f56c40ff142b8999d0606aa1908)\n"
     ]
    }
   ],
   "source": [
    "eli5 = load_dataset(\"rexarski/eli5_category\", split=\"train[:20]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c8b7eec-3c36-4edc-8dc6-8e6b32744c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'category', 'subreddit', 'answers', 'title_urls', 'selftext_urls'],\n",
       "        num_rows: 16\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'category', 'subreddit', 'answers', 'title_urls', 'selftext_urls'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5 = eli5.train_test_split(test_size=0.2)\n",
    "eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e060f4cb-7809-4641-8cf7-c9f490c00dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['q_id', 'title', 'selftext', 'category', 'subreddit', 'answers', 'title_urls', 'selftext_urls'],\n",
       "    num_rows: 16\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b57510e-430a-4c15-bc19-e904483ab39f",
   "metadata": {},
   "source": [
    "Let's now load a distilbert/distilgpt2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "348084a8-83c8-4e3d-a654-779e37aa5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75d09dfe-c40e-4237-90ee-72101b26a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1aa3f43-412e-4c69-a2e1-80c7a88524c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='distilbert/distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfb7fd64-0fd3-4aa3-a7fd-c8d35af811d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = eli5.flatten() # remove nestedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d11ace6-f4da-4845-a9d8-ce97affb0316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '5ld14u',\n",
       " 'title': 'Why are internet speeds in America so slow',\n",
       " 'selftext': '',\n",
       " 'category': 'Technology',\n",
       " 'subreddit': 'explainlikeimfive',\n",
       " 'title_urls': ['url'],\n",
       " 'selftext_urls': ['url'],\n",
       " 'answers.a_id': ['dburxe8',\n",
       "  'dbursr8',\n",
       "  'dbuuxnd',\n",
       "  'dbuutes',\n",
       "  'dbuy0vg',\n",
       "  'dbuxrff',\n",
       "  'dbuyci8',\n",
       "  'dbuy3lo',\n",
       "  'dbuxujv',\n",
       "  'dbuxyrh',\n",
       "  'dbuyev3',\n",
       "  'dbuy1ya',\n",
       "  'dbux4mj'],\n",
       " 'answers.text': [\"It mainly has to do with the monopoly that ISP's hold over different areas. Most people have one choice for their internet provider. It's not like you can switch to a different company if you aren't happy with their shit service. Another thing is that the capacity for fast internet exists it's just that they have no reason to give it to you. My city recently became a prospective city for google fiber and TWC immediately upgraded our 20mb/s connection (maximum available) to 100mb/s for free and also started offering 200mb/s and 300mb/s for not much extra. They're also doing everything they can lawsuit wise to keep google fiber out of the city\",\n",
       "  \"Verizon has the best coverage, but it's not the fastest, I'm visiting family in Atlanta and I get > 110Mbps on T-Mobile. Though, I'm on their Wi-Fi as [it's much faster]( URL_0 ) (he's an exec at HP so gotta have fast Internet). 90% of the answer is because they can. 10% of the answer is because money, America is huge, and dense, so having to have thousands/millions of cell towers spread across the country and having hundreds in a single city is expensive in regards to renting out the land, supplying enough data per tower, etc.\",\n",
       "  \"Of all of the countries to use as an example, you use Australia? They still have a good chunk of the country on dialup since it's such a large region but has only six or so metropolitan areas. Really, America's internet speeds aren't that bad. They're not #1 but I think they were in the top 10. Meanwhile, Australia is still trying to build a fiber network but its own government keeps sabotaging it\",\n",
       "  \"Several reasons: 1. there's little or no competition in most markets. 2. the cost of starting up puts it out of reach for most companies. Even Google is growing insanely slow due to the cost of laying new lines 3. we've got other problems. My Internet is reliable (like 95%+ uptime) and fast enough (50 Mb) for just about anything my family of 4 needs for $65 a month. The only way things are going to change is if we make a widespread political effort to change it and frankly, my efforts there are better spent on other issues.\",\n",
       "  \"Speeds are so slow because the telecom industry in America is a monopoly. In certain areas you only have one choice for a provider, so why would they spend money to improve their services if you don't have any other choice? The telecom industry has been vigorously lobbying and filing lawsuits to prevent competition. Things like poles, switches, and boxes (the kind you find outside) are owned and rented out. Lawsuits have been thrown around to prevent other companies from using those utilities which makes it hard to compete. Telecom companies also sign deals with the cities themselves. If the city attempts to provide their own internet they would get sued. The only reason Google Fiber was able to become a thing is because Google could afford to build their own utilities and fight in court with a chance of winning. The American telecom industry is the poster child of corruption and bad morals and ethics. They are not satisfied with their 97% profit margins and don't care one bit about making the world a better, more connected place. It is because of Google Fiber that companies like Comcast are finally starting to improve their networks, because it is clear to both sides that the moment Google Fiber is available everybody will jump ship. These companies are fully aware of what they are doing and how damaging they are and are terrified that their monopoly will end.\",\n",
       "  \"Most people here simply do not understand how absurdly expensive running fiber is here in the US. A lot of it is all the codes and standards, environmental protections etc. Just to give you an example. When we tried to build a single fiber run to a local school, the cost was $75,000. We got an estimate for wiring the entire small city for fiber (every home) and it came out to about $35 million. That's for about 15,000 people. That's almost $2400 per person if every single one of those 15,000 signs up. That's just the fiber and equipment, no bandwidth to backhaul providers, etc. And this involves no big greedy corporations who just want to charge you a ton of money because they are evil and diabolical as is the common belief on Reddit. And in the real world, this notion that bandwidth costs nothing is simply untrue. Sure there are some companies that trade bandwidth. So long as they are both sending each other the same amount of data each way, they don't charge each other for that mutual bandwidth. But its not very common for ISPs. The US is very vast and has lot of mountains in many areas. Those long distances cost a huge amount of money to cover, as do areas like mountains. So take the endless claims that it's all about big greedy corporations with a grain of salt. Feel free to do some research and see how much it would cost you if you were to build your own fiber network non-profit and you will see for yourself.\",\n",
       "  \"You won't get 200Mbps on Telstra LTE most of the time unless you're very lucky. First of all, the guy is in Melbourne, speedtesting to a Telstra server. Telstra has its HQ in Melbourne, and they have very strong mobile reception here (plus in other cities). But outside of the bigger cities, or even in the suburbs, the mobile cells are further spaced out, so there's more congestion and generally the speeds drop off a bit. It is pretty good though, I'm currently getting 85Mbps inside my house (Melbourne suburbs, about 8km from the CBD). Telstra is a world leader in LTE technology, they're investing a lot of money in it. If you moved to another carrier, you wouldn't get as good service. However the fixed line internet in Australia is nowhere near as good, as other commenters have said. Also, mobile data is expensive here, especially with Telstra (best network, so they charge more). You get 10GB on a $120/month plan with an iPhone 7 (unlimited talk and text). And when you go over the 10GB, it doesn't throttle your speed, you get charged at a $10/GB rate. Source: I'm currently doing an internship at Telstra :)\",\n",
       "  'LOL, sure you can get those speeds in Aus, but i mean you have a max of like 25GB before you start paying $10 per GB over that. Source: Am an internet seller man',\n",
       "  \"America is where the technology is developed. As a result it's usually the first country to get things and gets technology 1.0. As the technology gets deployed people figure improvements, better ways to do stuff, upgrades, etc and technology 2.0 is developed. Usually this updated technology is deployed in the rest of the world after the USA. So why don't you just upgrade the technology? Well, upgrading usually costs more money, and technology 1.0 tends to be good enough. BTW, this doesn't just refer to LTE cell phones, but to credit card chips (which most of the world had before us), and other stuff that's deployed first to the US and later to the world.\",\n",
       "  \"I work for T-Mobile as a project manager. We are the fastest in our area and many others but it really depends on the spectrum owned by the carrier. The US government (FCC) Is in charge of dividing up the available spectrum via auctions. 5MHZ of additional spectrum can cost billions of dollars per market (geographic area). That's the reason he carriers are always trying to merge or buy each other out. It isn't the towers, customers, or equipment. It's the spectrum. The carrier with the most bandwidth owns the speed and thus owns the customers. It's a vicious game.\",\n",
       "  \"There's an Adam Ruins Everything on this. If I remember correctly the TLDR is that since the govt thought that I'd all be just a fad, they didn't stop monopalization- There's barely any competition so no pressure to get any better.\",\n",
       "  'Collusion. Companies spend the absolute bare minimum they need to maintain their networks and strike deals amongst themselves so as not to have to compete with anyone else. When someone like Google Fiber comes into the market, they use lawyers and politicians to help keep them out and protect their fiefdoms.',\n",
       "  'The geographical area that is covered in other countries is is tiny compared to in the US. Even in countries that are larger than 1/50th of the US, they still only supply internet to their larger cities and metropolitan areas. Also, CDMA is an anti-competitive technology. Verizon is a CDMA company in the US. Everywhere else, GSM (Like T-Mobile and AT & T) is mandated and pretty much open to competitive forces which improve service and decrease consumer costs.'],\n",
       " 'answers.score': [423, 321, 235, 52, 22, 13, 9, 8, 8, 6, 3, 3, 3],\n",
       " 'answers.text_urls': [[],\n",
       "  ['http://www.speedtest.net/my-result/i/1916489877'],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  []]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60b2ddd1-70b1-4c3f-9cea-bec672fbd69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eli5['train'][0]['answers.text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324fcf08-f262-436d-baf2-3e22a345c8dc",
   "metadata": {},
   "source": [
    "let's join all list of strings into single string \\\n",
    "Joining a List of Strings: \\\n",
    "example:\n",
    "```python\n",
    "a = ['Hello', 'world', 'from', 'Python']\n",
    "res = '***'.join(a)\n",
    "res\n",
    "```\n",
    "result: \n",
    "```bash\n",
    "'Hello***world***from***Python'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91069a4e-661e-4cc7-b93c-0582ab053cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \" \".join(eli5['train'][9]['answers.text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34d619f8-78bc-4afc-bd26-11dbdc293887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1591 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2215, 345, 1011, 1165, 1263, 257, 26633, 11, 340, 23687, 534, 1658, 2522, 31111, 11, 6666, 340, 284, 599, 8597, 13, 770, 20406, 13, 383, 13349, 318, 3375, 546, 734, 1180, 1243, 13, 554, 262, 3670, 11, 356, 389, 1085, 284, 1975, 326, 262, 13349, 318, 3375, 546, 366, 301, 1186, 10813, 534, 1658, 2522, 31111, 11, 6666, 340, 284, 599, 8597, 1, 2102, 11, 287, 262, 2912, 6903, 11, 13349, 318, 3375, 546, 366, 1858, 389, 734, 1688, 366, 83, 29080, 1, 326, 534, 5422, 5983, 656, 13, 1881, 318, 262, 1658, 2522, 31111, 11, 981, 262, 584, 318, 262, 491, 4891, 64, 526, 8975, 340, 20406, 772, 618, 262, 4144, 1816, 866, 262, 366, 3506, 1, 12403, 13, 770, 4325, 749, 1690, 351, 6588, 515, 11758, 13, 383, 2793, 3833, 2354, 286, 262, 460, 14, 10985, 293, 290, 262, 4894, 286, 534, 1767, 1838, 6588, 17556, 284, 1282, 503, 286, 262, 20584, 14, 12924, 14, 1073, 365, 355, 345, 4144, 340, 13, 770, 318, 644, 5640, 262, 277, 6457, 1359, 1245, 13, 632, 338, 3608, 290, 25103, 355, 890, 355, 340, 4325, 287, 534, 5422, 13, 887, 644, 611, 262, 277, 6457, 1359, 7622, 1016, 319, 706, 345, 1053, 27961, 30, 3406, 1658, 2522, 31111, 373, 10629, 284, 5412, 281, 25799, 286, 8122, 11, 475, 6451, 340, 468, 281, 25799, 286, 8122, 1635, 392, 9, 257, 1588, 6115, 286, 3623, 287, 340, 13, 632, 338, 355, 611, 345, 27961, 257, 825, 17249, 21190, 290, 340, 7599, 32387, 2346, 981, 345, 547, 45590, 340, 0, 770, 5640, 12132, 20880, 286, 262, 1658, 2522, 31111, 13, 685, 18378, 60, 16263, 11, 428, 318, 517, 1884, 284, 1645, 618, 262, 4144, 5818, 470, 550, 1576, 640, 284, 277, 44461, 503, 878, 345, 26633, 11, 588, 618, 345, 4144, 3892, 503, 286, 257, 29026, 4721, 11, 4692, 460, 319, 257, 3024, 3931, 1110, 13, 1081, 314, 1833, 340, 11, 534, 1658, 2522, 31111, 2499, 287, 32969, 351, 534, 5422, 284, 640, 618, 340, 2476, 284, 923, 29148, 290, 28175, 284, 1445, 2057, 656, 534, 11384, 13, 11382, 286, 428, 2223, 588, 3867, 257, 49773, 422, 262, 4220, 286, 534, 16162, 34274, 9290, 284, 262, 4756, 1262, 1111, 2832, 2427, 286, 530, 13, 1649, 345, 26633, 366, 36460, 1600, 345, 1244, 5552, 617, 1633, 2427, 286, 262, 20584, 379, 717, 11, 543, 5640, 617, 12916, 3833, 319, 534, 1658, 2522, 31111, 355, 340, 6100, 262, 1633, 866, 656, 534, 11384, 4058, 286, 262, 24968, 13, 10776, 4003, 345, 460, 1577, 2495, 2511, 4356, 862, 706, 45590, 366, 36460, 1, 588, 326, 11, 1165, 30, 317, 1256, 286, 32805, 287, 428, 4704, 986, 632, 468, 2147, 284, 466, 351, 1660, 1016, 866, 262, 2642, 12403, 13, 1867, 4325, 318, 326, 618, 345, 7685, 26633, 11, 345, 22118, 257, 2168, 286, 27014, 2775, 507, 287, 534, 13589, 477, 262, 835, 866, 284, 534, 2793, 1658, 2522, 496, 282, 599, 71, 4612, 263, 13, 2312, 2775, 507, 481, 7685, 670, 284, 4574, 2057, 14, 7109, 676, 866, 262, 1658, 2522, 31111, 656, 262, 11384, 357, 5661, 318, 1521, 345, 460, 26633, 17196, 866, 290, 407, 423, 2057, 14, 7050, 1282, 503, 286, 534, 5422, 737, 11382, 286, 40237, 257, 613, 64, 503, 286, 257, 14787, 284, 38350, 703, 340, 2499, 13, 1649, 345, 366, 2032, 12154, 2642, 1600, 644, 4325, 318, 326, 262, 8280, 2775, 507, 11, 2427, 286, 2263, 1295, 826, 2029, 262, 11572, 385, 284, 4574, 340, 866, 11, 1682, 2753, 1295, 379, 262, 11572, 385, 11, 6666, 2356, 355, 262, 12749, 21229, 262, 11572, 385, 11, 2427, 286, 7796, 340, 13, 1318, 389, 734, 19860, 287, 534, 13589, 25, 530, 329, 2057, 14, 7109, 676, 290, 530, 329, 1633, 13, 1649, 345, 7685, 26633, 11, 534, 1633, 12403, 20612, 572, 290, 2057, 14, 7109, 676, 2925, 656, 534, 2057, 12656, 13, 4930, 1243, 460, 467, 2642, 284, 2728, 281, 22029, 264, 274, 25729, 25, 352, 8, 2057, 14, 7109, 676, 14170, 534, 1633, 12403, 416, 5778, 11, 1642, 345, 22094, 290, 4328, 10381, 11, 428, 318, 262, 1767, 338, 3288, 2882, 284, 1805, 534, 21726, 13, 362, 8, 262, 2057, 14, 7109, 676, 2925, 656, 534, 2057, 12656, 11, 475, 1223, 6622, 340, 510, 290, 340, 4329, 7819, 11, 3756, 284, 2356, 14, 6381, 21598, 13, 8090, 25, 8366, 24709, 10644, 7451, 2041, 1710, 287, 45590, 11916, 13, 1318, 389, 734, 1688, 366, 83, 29080, 1, 326, 534, 5422, 5983, 656, 13, 1881, 318, 262, 1658, 2522, 31111, 11, 981, 262, 584, 318, 262, 491, 4891, 64, 13, 383, 1658, 2522, 31111, 318, 810, 9013, 290, 38236, 467, 13, 383, 491, 4891, 64, 5983, 284, 534, 21726, 13, 1649, 345, 26633, 1223, 11, 257, 7009, 37699, 1444, 262, 2462, 38686, 1252, 271, 15174, 2057, 13166, 422, 1016, 656, 262, 491, 4891, 64, 13, 2102, 11, 3360, 428, 1595, 470, 1464, 670, 11, 290, 2057, 290, 38236, 481, 467, 866, 262, 2642, 12656, 357, 20424, 262, 491, 4891, 64, 2427, 286, 262, 1658, 2522, 31111, 737, 770, 5640, 10927, 24133, 274, 11, 884, 355, 48308, 290, 39302, 11, 284, 651, 883, 13166, 503, 13, 1114, 4469, 11, 314, 1101, 257, 604, 400, 614, 6623, 287, 30972, 349, 560, 782, 1435, 11, 523, 428, 318, 826, 510, 616, 23978, 13, 1675, 717, 1833, 534, 1808, 517, 4084, 11, 1309, 338, 8160, 257, 1178, 1243, 13, 46876, 5902, 544, 25, 19327, 286, 26633, 10529, 2047, 2522, 363, 544, 25, 2356, 351, 26633, 1081, 10514, 25, 2057, 14, 39250, 14, 21680, 12151, 8218, 534, 491, 4891, 64, 1081, 10514, 8833, 618, 345, 423, 617, 2099, 286, 19327, 286, 534, 300, 38621, 87, 357, 301, 5620, 13160, 286, 290, 6493, 12435, 45173, 326, 7662, 689, 1141, 26633, 8, 290, 2462, 38686, 1252, 271, 357, 26674, 50006, 37699, 326, 20612, 625, 534, 300, 38621, 87, 284, 1805, 1633, 1014, 618, 45590, 737, 1081, 10514, 2728, 345, 284, 22094, 13, 27095, 318, 407, 12132, 13, 770, 318, 644, 4325, 618, 661, 910, 366, 270, 1816, 866, 262, 2642, 12656, 526, 632, 460, 1716, 281, 2071, 618, 340, 5983, 284, 48217, 35647, 13, 35527, 7974, 290, 24109, 357, 273, 584, 3925, 3025, 21726, 290, 2344, 34360, 2314, 6436, 453, 1598, 30322, 385, 8, 460, 1205, 281, 10280, 286, 262, 21726, 611, 11492, 4477, 284, 3802, 428, 1989, 290, 318, 407, 12539, 6105, 13, 46876, 5902, 544, 460, 307, 9086, 510, 656, 257, 1178, 1180, 9376, 13, 42222, 532, 7411, 262, 5422, 11, 477, 16171, 8280, 3356, 13, 21561, 286, 2130, 351, 257, 36175, 11880, 2111, 284, 4574, 257, 2057, 11572, 385, 284, 262, 736, 286, 511, 5422, 13, 9576, 2408, 13, 440, 10051, 38621, 469, 282, 532, 257, 6087, 286, 16171, 290, 37099, 8280, 3356, 13, 770, 7108, 286, 45590, 318, 6209, 534, 4075, 26633, 30122, 6268, 3940, 416, 37099, 6268, 286, 872, 38621, 469, 282, 12749, 13, 15624, 306, 351, 4939, 872, 38621, 469, 282, 12749, 11, 393, 2130, 508, 468, 6989, 257, 14000, 13891, 428, 1989, 13, 12032, 11, 379, 428, 3800, 11, 611, 262, 2462, 38686, 1252, 271, 10143, 284, 15350, 6105, 11, 48217, 460, 3051, 13, 8678, 2522, 496, 282, 532, 3190, 37099, 8280, 3356, 13, 46876, 5902, 544, 287, 428, 7108, 460, 307, 10345, 393, 12370, 13, 19663, 8833, 611, 612, 318, 617, 2099, 286, 28118, 287, 262, 1658, 2522, 31111, 11, 588, 257, 5858, 11, 3992, 11, 393, 22359, 13, 770, 2099, 286, 13147, 5902, 544, 8833, 8384, 351, 4735, 2057, 290, 407, 8122, 13, 44224, 50111, 363, 544, 8833, 611, 612, 318, 617, 2099, 286, 18992, 8280, 6268, 11, 588, 2793, 1658, 2522, 496, 282, 599, 71, 4612, 263, 599, 8597, 393, 42864, 1658, 2522, 496, 282, 599, 8597, 13, 632, 8833, 351, 4735, 290, 8122, 13, 20759, 612, 318, 2035, 257, 42087, 20781, 2984, 32560, 1871, 262, 25377, 393, 257, 10726, 1917, 13, 1439, 777, 3858, 286, 50111, 363, 544, 460, 1944, 351, 2356, 287, 1180, 3006, 1912, 319, 262, 4067, 286, 262, 19327, 13, 27095, 287, 262, 3329, 11, 257, 1790, 5615, 1067, 696, 14, 35436, 351, 45590, 318, 1884, 2233, 284, 281, 1658, 2522, 496, 282, 599, 8597, 13, 1649, 257, 24968, 318, 27961, 366, 36460, 1, 4143, 340, 318, 2233, 284, 257, 10000, 286, 1633, 287, 262, 13589, 852, 13640, 2174, 262, 1517, 852, 27961, 13, 770, 1633, 5640, 257, 4622, 517, 5894, 4136, 3264, 2174, 262, 2134, 852, 27961, 290, 4145, 356, 1254, 340, 517, 12034, 13, 770, 4203, 318, 6209, 2233, 284, 262, 10000, 286, 1633, 11, 475, 318, 635, 6196, 28310, 416, 257, 599, 8597, 286, 262, 13589, 13, 1318, 338, 257, 10649, 326, 2925, 422, 534, 5422, 284, 534, 11814, 1820, 11, 475, 340, 338, 3221, 4838, 572, 416, 257, 1310, 8946, 13, 1649, 345, 821, 7722, 13135, 11, 262, 8946, 9808, 523, 326, 35161, 1869, 460, 10649, 866, 284, 534, 11814, 1820, 13, 1002, 345, 821, 407, 5989, 1576, 3241, 618, 345, 821, 7722, 11, 262, 17435, 284, 262, 10649, 481, 6044, 284, 1280, 13, 1406, 618, 35161, 1869, 18045, 329, 262, 10649, 11, 339, 45846, 826, 832, 262, 8946, 13, 1320, 338, 1521, 340, 20406, 618, 345, 26633, 2642, 13, 383, 12749, 287, 534, 13589, 36755, 284, 3613, 345, 422, 340, 1682, 1016, 866, 262, 2642, 12403, 13, 1675, 466, 326, 11, 326, 484, 423, 284, 466, 257, 1256, 517, 670, 621, 3487, 13, 843, 326, 7584, 8468, 3833, 319, 262, 25377, 287, 262, 1989, 290, 3863, 772, 374, 2419, 262, 12749, 2950, 257, 1643, 492, 1111, 286, 883, 561, 2728, 2356, 326, 6296, 503, 422, 3487, 34129, 11, 290, 561, 307, 3538, 6810, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffbcbe80-865d-4fd5-ac59-eeae7e07ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad91fc82-a22b-4752-8018-72b854ae62ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f908f0c3d57448f6b3de01a7e9356bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5eb131f62247c3a0f2b517a44522a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b046ce9de86e4c99a6d9acc0606f3eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4e102359d446b19348b748c5240fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dev/lib/python3.11/site-packages/datasets/table.py:1318: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6638b5d34aee4e96babbc621aed292ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72d46413a154857844a9c5366bd2eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8d29c4e30848e580006c68cfaf800d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173fea9a57804b8fac181407d31beebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dev/lib/python3.11/site-packages/datasets/table.py:1318: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4c903cf-fdf5-4639-83a7-360788aaf2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 16\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eli5['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21358292-d527-43bc-a83f-a74efbe0a5d5",
   "metadata": {},
   "source": [
    "This dataset contains the token sequences, but some of these are longer than the maximum input length for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b246c-b2b2-4507-9b18-84d2989b06d6",
   "metadata": {},
   "source": [
    "now use a second preprocessing function to\n",
    "\n",
    "- concatenate all the sequences \n",
    "- split the concatenated sequences into shorter chunks defined by block_size, which should be both shorter than the maximum input length and short enough for your GPU RAM.\n",
    "\n",
    "concatenate\n",
    "```python\n",
    "a = [[12,], [13, 15], [12]]\n",
    "[ele for lst in a for ele in lst]\n",
    "```\n",
    "```bash\n",
    "[12, 13, 15, 12]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0c1eba01-b1fa-480b-b7a5-2a12c66762d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {}\n",
    "    for k in examples.keys():\n",
    "        concatenated_examples[k] = [ele for lst in examples[k] for ele in lst]\n",
    "    total_length = len(concatenated_examples[list(concatenated_examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b58237a1-aa51-477d-9328-79cfe1d98e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5736f3c20bd40e297cd01e1beda1b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c4fc0cc6d94f01a8fde86e80635eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0909fcee-95aa-48db-89fd-d1e23ed93bc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(lm_dataset['train'][0]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c590b0e-2483-4db4-a393-455b2638cf91",
   "metadata": {},
   "source": [
    "Now create a batch of examples using DataCollatorForLanguageModeling. Itâ€™s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n",
    "\n",
    "In the ðŸ¤— Transformers library, a data collator is a helper that prepares a batch of data just before feeding it into the model during training or evaluation.\n",
    "\n",
    "â¸»\n",
    "\n",
    "ðŸ§  Why itâ€™s needed\n",
    "\n",
    "Models expect input tensors of the same length, but natural language sequences are different lengths.\n",
    "\n",
    "So we need to:\n",
    "- Pad shorter sequences\n",
    "- Possibly create labels\n",
    "- Stack inputs into tensors\n",
    "\n",
    "Instead of doing this manually every time, we use a data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dd1f70d1-8d5e-490d-ac77-23cfdc64484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2356b9bf-d588-4df4-a4d2-5522a58b52dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='distilbert/distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b14a0923-8aca-449d-8cbd-41d421333aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5019902e-3195-4b03-9d8d-d4523f1f439d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='distilbert/distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "03185ac5-c51b-40d4-a5d1-368ee88126a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf643b8-e6b6-4f0b-a76d-2258c3404307",
   "metadata": {},
   "source": [
    "Load DistilGPT2 with AutoModelForCausalLM \n",
    "\n",
    "At this point, only three steps remain:\n",
    "\n",
    "- Define your training hyperparameters in TrainingArguments. The only required parameter is output_dir which specifies where to save your model. Youâ€™ll push this model to the Hub by setting push_to_hub=True (you need to be signed in to Hugging Face to upload your model).\n",
    "- Pass the training arguments to Trainer along with the model, datasets, and data collator.\n",
    "- Call train() to finetune your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "728095fd-17b9-4373-bb53-d753ebbc2169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "392222a3-1f49-4495-bdbd-5ae373f435d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c5e071ca-50af-4bda-9f8e-2095624b2cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "70e5110e-c813-4c54-b417-86ad6f75da57",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_eli5_clm-model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ad67ff8a-bb49-4360-912e-cd55e5248091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/30/sjym4ttn64d9qz2ldbb6dblr0000gn/T/ipykernel_57119/3383620556.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "269f196c-9c20-466c-99e5-6391c418d277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.977015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8, training_loss=3.9569272994995117, metrics={'train_runtime': 43.0267, 'train_samples_per_second': 1.441, 'train_steps_per_second': 0.186, 'total_flos': 2025049817088.0, 'train_loss': 3.9569272994995117, 'epoch': 1.0})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "59bcd35d-cdab-405d-bf4b-077527987f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 53.36\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "71db9226-f89e-4957-a789-6e3c24590ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Somatic hypermutation allows the immune system to generate many different types of neurons. For example, it's necessary for a certain type of neuron to synthesize two different types of cells, the type they use, and the type, the type the\"}]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "prompt = \"Somatic hypermutation allows the immune system to\"\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8ddcc-5345-4a1c-a742-474ac3cbb545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
